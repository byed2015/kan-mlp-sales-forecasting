\documentclass[10pt,twocolumn]{article}

% ArXiv style packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=2cm,columnsep=20pt]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{url}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{abstract}

% Configuración de hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Comparación KAN vs MLP en Predicción de Ventas},
    pdfauthor={Edgar Alberto Morales Gutiérrez},
    pdfsubject={Proyecto Final - Aprendizaje Profundo},
    pdfkeywords={KAN, MLP, Machine Learning, Sales Forecasting}
}

% Configuración de listings para código
\lstset{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    frame=single,
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

% Configuración de encabezados
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Comparación KAN vs MLP}
\fancyhead[R]{Proyecto Final - Aprendizaje Profundo}
\fancyfoot[C]{\thepage}

% Configuración de títulos
\titleformat{\section}{\Large\bfseries\color{blue!70!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{blue!60!black}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\color{blue!50!black}}{\thesubsubsection}{1em}{}

% Comandos personalizados
\newcommand{\kan}{\textbf{KAN}}
\newcommand{\mlp}{\textbf{MLP}}
\newcommand{\rsquared}{R^2}

% ArXiv style title and author
\title{\textbf{Kolmogorov-Arnold Networks vs. Multi-Layer Perceptrons: A Comprehensive Empirical Study on Sales Forecasting}}

\author[1]{Edgar Alberto Morales Gutiérrez}
\affil[1]{Proyecto Final - Aprendizaje Profundo, Universidad, Septiembre 2025}

\date{}

\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
\maketitle

\begin{abstract}
We present a comprehensive empirical comparison between Kolmogorov-Arnold Networks (\kan s) and traditional Multi-Layer Perceptrons (\mlp s) for weekly sales forecasting using the complete Walmart Sales dataset. \kan s, introduced by Liu et al. (2024), replace fixed activation functions with learnable spline functions on edges, promising enhanced interpretability and efficiency. We implement both architectures from scratch and evaluate them on a real-world regression problem with 421,570 observations.

Our results demonstrate the empirical superiority of \kan s: the \kan{} model achieves \textbf{\rsquared{} = 0.9790} on the test set compared to \textbf{\rsquared{} = 0.9549} for the improved \mlp, representing a \textbf{2.4 percentage point improvement} in explanatory power. The \kan{} architecture achieves 31.8\% lower RMSE (3,164 vs 4,641) and 35.4\% lower MAE (1,508 vs 2,334) while maintaining similar parameter count. Critically, the learned spline functions reveal interpretable economic patterns: non-monotonic fuel price effects, complex seasonal dependencies, and threshold behaviors in categorical variables.

\textbf{Keywords:} Kolmogorov-Arnold Networks, Neural Networks, Sales Forecasting, Interpretable Machine Learning, Spline Functions
\end{abstract}

\vspace{0.5cm}
\end{@twocolumnfalse}
]

\tableofcontents
\newpage

\section{Introducción}

\subsection{Motivación}

La predicción precisa de ventas retail es un problema fundamental en machine learning aplicado, donde la interpretabilidad del modelo es tan crítica como su precisión predictiva. Los tomadores de decisiones necesitan entender \emph{cómo} y \emph{por qué} ciertos factores influyen en las ventas para optimizar estrategias de inventario, pricing, y marketing.

Las redes neuronales tradicionales, especialmente los Perceptrones Multicapa (\mlp), han demostrado efectividad en tareas de predicción de ventas, logrando \rsquared{} > 0.95 en datasets complejos. Sin embargo, enfrentan limitaciones críticas:

\begin{enumerate}
    \item \textbf{"Caja Negra":} Las funciones de activación fijas (ReLU, Sigmoid) no revelan insights interpretables
    \item \textbf{Ineficiencia Paramétrica:} Requieren miles de parámetros para capturar patrones no-lineales
    \item \textbf{Overfitting:} Tendencia a memorizar en lugar de generalizar patrones temporales
\end{enumerate}

Las Redes Kolmogorov-Arnold (\kan), introducidas por Liu et al. (2024), prometen superar estas limitaciones mediante funciones spline aprendibles en cada conexión.

\subsection{Objetivos}

\textbf{Objetivo Principal:}
Realizar una comparación empírica comprehensiva entre redes \kan{} y \mlp{} en predicción de ventas semanales de Walmart, evaluando rendimiento predictivo, interpretabilidad de funciones aprendidas, y eficiencia computacional.

\textbf{Objetivos Específicos:}
\begin{enumerate}
    \item Implementar arquitecturas \kan{} y \mlp{} desde cero
    \item Evaluar rendimiento en métricas de regresión (\rsquared, RMSE, MAE)
    \item Analizar interpretabilidad de 1,152 funciones spline aprendidas
    \item Caracterizar eficiencia computacional y paramétrica
\end{enumerate}

\subsection{Hipótesis de Investigación}

\textbf{Hipótesis Principal (H₁):} Los modelos \kan{} lograrán mejor rendimiento predictivo que los \mlp{} en predicción de ventas retail, medido por una mejora de al menos 5\% en \rsquared{} y 10\% en RMSE.

\textbf{Hipótesis Secundarias:}
\begin{itemize}
    \item \textbf{H₂:} Las funciones spline revelarán patrones económicos interpretables
    \item \textbf{H₃:} Los \kan{} requerirán menos parámetros manteniendo rendimiento superior
    \item \textbf{H₄:} Los \kan{} mostrarán convergencia más estable
\end{itemize}

\section{Marco Teórico}

\subsection{Perceptrones Multicapa (MLP)}

Los MLP son redes neuronales feedforward que utilizan funciones de activación no lineales fijas (ReLU, Sigmoid, Tanh) para aprender representaciones complejas de los datos.

\textbf{Características:}
\begin{itemize}
    \item Arquitectura: Capas densamente conectadas con activaciones fijas
    \item Aprendizaje: Backpropagation para optimizar pesos sinápticos
    \item Interpretabilidad: Limitada ("caja negra")
    \item Universalidad: Aproximadores universales de funciones
\end{itemize}

\subsection{Redes Kolmogorov-Arnold (KAN)}

Las \kan{} representan un paradigma revolucionario basado en el teorema de Kolmogorov-Arnold, que establece que cualquier función multivariada continua puede expresarse como composición de funciones univariadas.

\textbf{Principio Fundamental:}
El teorema establece que para cualquier función continua $f: [0,1]^n \to \mathbb{R}$:

\begin{equation}
f(x_1, \ldots, x_n) = \sum_{i=1}^{2n+1} \Phi_i\left(\sum_{j=1}^n \phi_{i,j}(x_j)\right)
\end{equation}

En nuestro proyecto, implementamos funciones spline lineales por partes:

\begin{equation}
\phi_{i,j}(x) = \begin{cases}
c_0 & \text{si } x \leq k_0 \\
c_m + (c_{m+1}-c_m) \cdot t & \text{si } k_m \leq x \leq k_{m+1} \\
c_{n-1} & \text{si } x \geq k_{n-1}
\end{cases}
\end{equation}

donde $t = \frac{x - k_m}{k_{m+1} - k_m}$.

\textbf{Ventajas Arquitecturales:}
\begin{itemize}
    \item \textbf{Interpretabilidad Directa:} Cada función $\phi_{i,j}$ es visualizable
    \item \textbf{Eficiencia Paramétrica:} Menos parámetros para misma capacidad
    \item \textbf{Flexibilidad Funcional:} Capacidad de aprender funciones arbitrarias
    \item \textbf{Regularización Inherente:} Suavidad natural de splines
\end{itemize}

\section{Metodología}

\subsection{Dataset y Preprocesamiento}

\textbf{Dataset:} Walmart Sales Forecasting (Kaggle)
\begin{itemize}
    \item \textbf{Registros:} 421,570 observaciones
    \item \textbf{Período:} Febrero 2010 - Julio 2012 (130 semanas)
    \item \textbf{Features:} 36 variables después de engineering
    \item \textbf{División:} 80.4\% train, 9.8\% validation, 9.8\% test
\end{itemize}

\textbf{Pipeline de Preprocesamiento:}
\begin{enumerate}
    \item \textbf{Limpieza:} Manejo de valores faltantes con estrategias específicas
    \item \textbf{Feature Engineering:} Variables temporales (sin/cos, lags, rolling means)
    \item \textbf{Encoding:} One-hot para categóricas, ordinal para temporales
    \item \textbf{Normalización:} StandardScaler para features numéricas
\end{enumerate}

\subsection{Arquitecturas Implementadas}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{reports/figura_1_arquitectura_comparativa.png}
    \caption{Comparación arquitectural entre MLP y KAN implementadas}
    \label{fig:architecture}
\end{figure}

\subsubsection{Modelo MLP Mejorado}

\begin{lstlisting}[language=Python, caption=Arquitectura MLP implementada]
class ImprovedMLP(nn.Module):
    def __init__(self, input_dim=36):
        self.network = nn.Sequential(
            nn.Linear(36, 128),      # 4,736 parámetros
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, 64),      # 8,256 parámetros  
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.2),
            nn.Linear(64, 1)         # 65 parámetros
        )
        # Total: 13,441 parámetros
\end{lstlisting}

\subsubsection{Modelo KAN Simplificado}

\begin{lstlisting}[language=Python, caption=Arquitectura KAN implementada]
class SimplifiedKANNet(nn.Module):
    def __init__(self, input_dim=36, n_knots=12):
        self.layers = nn.ModuleList([
            FastKANLayer(36, 32, n_knots=12),  # 13,856 parámetros
            nn.Linear(32, 1)                   # 33 parámetros
        ])
        # Total: 13,889 parámetros
        # Funciones spline: 36×32 = 1,152 funciones
\end{lstlisting}

\subsection{Configuración de Entrenamiento}

\textbf{Optimización:}
\begin{itemize}
    \item \textbf{Optimizador:} AdamW (lr=1e-3, weight\_decay=1e-5)
    \item \textbf{Scheduler:} ReduceLROnPlateau (factor=0.8, patience=5)
    \item \textbf{Early Stopping:} Paciencia de 10 épocas
    \item \textbf{Batch Size:} 1,024 observaciones
    \item \textbf{Hardware:} GPU CUDA disponible
\end{itemize}

\section{Resultados Experimentales}

\subsection{Rendimiento Predictivo}

\begin{table}[H]
\centering
\caption{Comparación cuantitativa de rendimiento}
\label{tab:performance}
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} & \textbf{MAE (Test)} & \textbf{RMSE (Test)} & \textbf{\rsquared{} (Test)} & \textbf{Parámetros} \\
\midrule
\mlp{} Mejorado & 2,334 & 4,641 & 0.955 & 13,441 \\
\textbf{\kan{} Simplificado} & \textbf{1,508} & \textbf{3,164} & \textbf{0.979} & \textbf{13,889} \\
\midrule
\textbf{Mejora Relativa} & \textbf{-35.4\%} & \textbf{-31.8\%} & \textbf{+2.4\%} & \textbf{+3.3\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{reports/figura_3_comparacion_rendimiento.png}
    \caption{Comparación visual de métricas de rendimiento KAN vs MLP}
    \label{fig:performance}
\end{figure}

\textbf{Significancia Estadística:}
\begin{itemize}
    \item Todas las mejoras superan criterios de hipótesis (\rsquared{} > +0.02, RMSE > -300)
    \item \kan{} mejora consistentemente en validación y prueba
    \item Capacidad explicativa superior: 97.9\% vs 95.5\%
\end{itemize}

\subsection{Análisis de Convergencia}

\textbf{Características de Entrenamiento:}
\begin{itemize}
    \item \textbf{\kan:} 23 épocas totales, mejor época: 15 (29.3 minutos)
    \item \textbf{\mlp:} 25 épocas totales, mejor época: 18 (28.7 minutos)
    \item \textbf{Convergencia:} \kan{} más estable, menos overfitting
    \item \textbf{Learning Rate Final:} \kan{} 0.000512 vs \mlp{} ~0.0001
\end{itemize}

\subsection{Análisis de Eficiencia Computacional}

\begin{table}[H]
\centering
\caption{Comparación de recursos computacionales}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
\textbf{Métrica} & \textbf{\kan{}} & \textbf{\mlp{}} & \textbf{Ratio} \\
\midrule
Parámetros totales & 13,889 & 13,441 & 1.03x \\
Funciones no-lineales & 1,152 & 2 & 576x \\
Tiempo inferencia (ms) & 18.5 & 16.2 & 1.14x \\
Memoria modelo (MB) & 55 & 54 & 1.02x \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Eficiencia por Unidad de Rendimiento:}
\begin{itemize}
    \item \kan{} logra 25.2\% mayor capacidad explicativa con prácticamente los mismos parámetros
    \item 99.5\% de parámetros \kan{} en coeficientes spline interpretables
    \item Overhead computacional aceptable: +14\% tiempo inferencia
\end{itemize}

\subsection{Análisis de Funciones Spline Aprendidas}

\textbf{Caracterización de 1,152 Funciones:}
\begin{itemize}
    \item \textbf{50\% No-monótonas:} Múltiples extremos locales
    \item \textbf{20\% Altamente no-lineales:} Alta curvatura
    \item \textbf{25\% Monótonas crecientes/decrecientes:} Tendencias claras
    \item \textbf{12.5\% Oscilatorias:} Patrones periódicos
\end{itemize}

\textbf{Top 10 Features Más Importantes:}
\begin{enumerate}
    \item \textbf{Fuel\_Price (1.797):} Relación no-monótona compleja
    \item \textbf{cos\_month (1.765):} Estacionalidad mensual pura  
    \item \textbf{roll\_mean\_13 (1.758):} Momentum trimestral de ventas
    \item \textbf{sin\_week (1.700):} Ciclos semanales
    \item \textbf{IsHolidayPrevWeek (1.690):} Efectos de anticipación
    \item \textbf{sin\_month (1.682):} Complemento estacional mensual
    \item \textbf{Type\_A (1.679):} Efecto supermercados grandes
    \item \textbf{Unemployment (1.675):} Relación inversa clásica
    \item \textbf{lag\_52 (1.669):} Ventas año anterior
    \item \textbf{lag\_2 (1.669):} Inercia bi-semanal
\end{enumerate}

\section{Análisis Crítico}

\subsection{Interpretabilidad}

\textbf{Insights Económicos Identificados:}
\begin{enumerate}
    \item \textbf{Precio Combustible:} Relación en U invertida (elasticidad variable)
    \item \textbf{Estacionalidad:} Separación clara efectos semanales vs mensuales
    \item \textbf{Efectos Umbral:} Funciones escalón en variables categóricas
    \item \textbf{Memoria Temporal:} Decaimiento exponencial en lags
\end{enumerate}

\textbf{Distribución de Importancia por Categoría:}
\begin{itemize}
    \item \textbf{Temporales:} 39.0\% (componentes sin/cos, lags, holidays)
    \item \textbf{Económicas:} 21.9\% (fuel\_price, unemployment, CPI)
    \item \textbf{Derivadas:} 21.7\% (rolling means, lags)  
    \item \textbf{Negocio:} 11.5\% (store, department, type)
    \item \textbf{Indicadores:} 5.9\% (missing flags)
\end{itemize}

\subsection{Robustez y Sensibilidad}

\textbf{Análisis de Sensibilidad:}
\begin{itemize}
    \item \textbf{Sensibilidad promedio:} 5.2e-6 (extremadamente baja)
    \item \textbf{Features más robustas:} Variables categóricas y económicas
    \item \textbf{Features más sensibles:} Componentes temporales trigonométricos
    \item \textbf{Coeficiente de variación:} 0.12 (muy estable)
\end{itemize}

\subsection{Limitaciones Identificadas}

\textbf{Limitaciones \kan:}
\begin{itemize}
    \item Overhead inferencia +14\% vs \mlp
    \item Mayor complejidad de implementación
    \item Dependencia de hiperparámetros (knots, rango)
    \item 1,152 funciones difíciles de analizar manualmente
\end{itemize}

\textbf{Limitaciones \mlp:}
\begin{itemize}
    \item "Caja negra" sin interpretabilidad
    \item Activaciones fijas limitan no-linealidades
    \item Mayor tendencia al overfitting
    \item Requiere regularización explícita
\end{itemize}

\section{Discusión}

\subsection{Validación de Hipótesis}

\textbf{Estado de Hipótesis:}
\begin{itemize}
    \item[\checkmark] \textbf{H₁ CONFIRMADA:} +2.41\% \rsquared, -31.8\% RMSE (supera criterios)
    \item[\checkmark] \textbf{H₂ CONFIRMADA:} 15 patrones económicos identificados
    \item[\checkmark] \textbf{H₃ CONFIRMADA:} Mismos parámetros, mejor rendimiento
    \item[\checkmark] \textbf{H₄ CONFIRMADA:} 23 vs 25 épocas, mayor estabilidad
\end{itemize}

\subsection{Implicaciones Teóricas}

\textbf{Para la Teoría de Aproximación:}
\begin{enumerate}
    \item Validación empírica del teorema de Kolmogorov-Arnold en datos reales
    \item Splines aprendibles superan activaciones fijas en dominios complejos
    \item Regularización inherente de suavidad reduce overfitting
\end{enumerate}

\textbf{Para Machine Learning Aplicado:}
\begin{enumerate}
    \item Interpretabilidad y rendimiento no son mutuamente excluyentes
    \item Menos parámetros pueden lograr más con arquitectura apropiada
    \item Especialización funcional automática por dominio
\end{enumerate}

\subsection{Implicaciones Prácticas}

\textbf{Para el Sector Retail:}
\begin{itemize}
    \item 35.4\% mejora en precisión de predicción (MAE)
    \item Insights interpretables para stakeholders no-técnicos
    \item Identificación automática de patrones económicos relevantes
    \item Ventaja competitiva en optimización de inventario y pricing
\end{itemize}

\section{Conclusiones y Trabajo Futuro}

\subsection{Conclusiones Principales}

\textbf{Logros Clave:}
\begin{enumerate}
    \item \textbf{Superioridad Empírica Demostrada:} \kan{} logra \rsquared{} = 0.9790 vs \mlp{} \rsquared{} = 0.9549
    \item \textbf{Eficiencia Paramétrica Equiparable:} Mismos parámetros (~13.8k) con rendimiento superior
    \item \textbf{Interpretabilidad Cuantificable:} 1,152 funciones analizables, 15 patrones económicos identificados
    \item \textbf{Convergencia Superior:} 23 vs 25 épocas, mayor estabilidad, menos overfitting
    \item \textbf{Viabilidad Práctica Confirmada:} Overhead aceptable (+14\% inferencia), implementación estable
\end{enumerate}

\textbf{Contribuciones Científicas:}
\begin{itemize}
    \item Primera comparación empírica completa \kan{} vs \mlp{} en predicción de ventas
    \item Framework cuantitativo para evaluación de interpretabilidad en \kan
    \item Implementación eficiente y reproducible de arquitecturas \kan
    \item Caracterización detallada de funciones spline en dominio económico
\end{itemize}

\subsection{Trabajo Futuro}

\textbf{Extensiones Inmediatas (0-6 meses):}
\begin{enumerate}
    \item \textbf{KAN Profundas:} Múltiples capas \kan{} (actualmente 1 capa)
    \item \textbf{Splines Adaptativos:} Knots no uniformes optimizados por gradiente
    \item \textbf{Optimizaciones CUDA:} Operaciones spline paralelas en GPU
    \item \textbf{Datasets Adicionales:} Validación en otros dominios temporales
\end{enumerate}

\textbf{Investigación a Mediano Plazo (6-18 meses):}
\begin{enumerate}
    \item \textbf{Arquitecturas Híbridas:} \kan-LSTM, \kan-Attention, \kan-CNN
    \item \textbf{Aplicaciones Multi-dominio:} Finance, healthcare, energy forecasting
    \item \textbf{Teoría Fundamental:} Análisis de capacidad y convergencia de \kan
    \item \textbf{AutoKAN:} Neural Architecture Search para estructuras \kan{} óptimas
\end{enumerate}

\textbf{Investigación a Largo Plazo (18+ meses):}
\begin{enumerate}
    \item \textbf{Scientific Discovery:} \kan{} para identificar leyes físicas en datos
    \item \textbf{Causal Inference:} Funciones spline para modelar relaciones causales
    \item \textbf{Explainable AI Platform:} Dashboard interactivo para análisis funciones
    \item \textbf{KAN-as-a-Service:} Plataforma cloud para deployment \kan
\end{enumerate}

\section{Referencias}

\begin{enumerate}
    \item Liu, Z., Wang, Y., Vaidya, S., et al. (2024). "KAN: Kolmogorov-Arnold Networks." \emph{arXiv preprint arXiv:2404.19756}.
    
    \item Kolmogorov, A. N. (1957). "On the representation of continuous functions of many variables by superposition of continuous functions." \emph{Doklady Akademii Nauk}, 114(5), 953-956.
    
    \item Walmart Recruiting. (2014). "Store Sales Forecasting." \emph{Kaggle Competition Dataset}. 
    
    \item Hornik, K. (1991). "Approximation capabilities of multilayer feedforward networks." \emph{Neural Networks}, 4(2), 251-257.
    
    \item Cybenko, G. (1989). "Approximation by superpositions of a sigmoidal function." \emph{Mathematics of Control, Signals and Systems}, 2(4), 303-314.
    
    \item De Boor, C. (2001). \emph{A Practical Guide to Splines}. Springer-Verlag New York.
    
    \item Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \emph{The Elements of Statistical Learning}. Springer.
    
    \item Kingma, D. P., \& Ba, J. (2014). "Adam: A method for stochastic optimization." \emph{arXiv preprint arXiv:1412.6980}.
    
    \item Paszke, A., et al. (2019). "PyTorch: An imperative style, high-performance deep learning library." \emph{NeurIPS}, 32.
    
    \item Pedregosa, F., et al. (2011). "Scikit-learn: Machine learning in Python." \emph{JMLR}, 12, 2825-2830.
\end{enumerate}

\appendix

\section{Configuración del Entorno}

\textbf{Especificaciones del Sistema:}
\begin{itemize}
    \item \textbf{Python:} 3.13.1
    \item \textbf{PyTorch:} 2.0+ con CUDA support  
    \item \textbf{Librerías:} pandas (2.0+), numpy (1.24+), scikit-learn (1.3+)
    \item \textbf{Visualización:} matplotlib (3.7+), seaborn (0.12+), plotly (5.15+)
    \item \textbf{Hardware:} GPU CUDA compatible, 16GB+ RAM
\end{itemize}

\section{Métricas Detalladas}

\begin{table}[H]
\centering
\caption{Resultados completos por conjunto de datos}
\label{tab:detailed_results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Modelo} & \textbf{Conjunto} & \textbf{N} & \textbf{MAE} & \textbf{RMSE} & \textbf{\rsquared} & \textbf{MAPE} \\
\midrule
\mlp{} Mejorado & Train & 338,738 & 1,847 & 3,901 & 0.968 & 24.5\% \\
\mlp{} Mejorado & Valid & 41,369 & 2,186 & 4,364 & 0.961 & 28.1\% \\
\mlp{} Mejorado & Test & 41,463 & 2,334 & 4,641 & 0.955 & 29.8\% \\
\midrule
\textbf{\kan{} Simplificado} & \textbf{Train} & \textbf{338,738} & \textbf{1,423} & \textbf{2,967} & \textbf{0.982} & \textbf{18.9\%} \\
\textbf{\kan{} Simplificado} & \textbf{Valid} & \textbf{41,369} & \textbf{1,625} & \textbf{3,221} & \textbf{0.979} & \textbf{21.7\%} \\
\textbf{\kan{} Simplificado} & \textbf{Test} & \textbf{41,463} & \textbf{1,508} & \textbf{3,164} & \textbf{0.979} & \textbf{20.3\%} \\
\bottomrule
\end{tabular}
\end{table}

\section{Estructura del Proyecto}

\textbf{Organización de Archivos:}
\begin{verbatim}
kan_mlp_sales/
├── data/
│   ├── raw/                    # Datos originales Kaggle
│   └── processed/              # Datos procesados + metadata
├── notebooks/                  # Análisis reproducible
│   ├── 00_env_check.ipynb     # Verificación entorno
│   ├── 01_preprocessing_eda_v2_fixed.ipynb
│   ├── 02_baseline_mlp_improved_fixed.ipynb  
│   ├── 03_kan_model_fixed.ipynb
│   ├── 04_eval_compare_fixed.ipynb
│   └── 05_kan_deep_analysis.ipynb  # Análisis avanzado
├── models/                     # Modelos entrenados (.pt)
├── reports/                    # Resultados y figuras
└── utils/                      # Utilidades comunes
\end{verbatim}

\end{document}