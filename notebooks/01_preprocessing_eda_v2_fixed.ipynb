{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5012c273",
   "metadata": {},
   "source": [
    "# 01_preprocessing_eda_v2 — Robust EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a501ca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_RAW: c:\\Users\\byed2\\Documents\\miacd\\Aprendizaje Profundo\\Proyecto Final\\kan_mlp_sales\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports & paths\n",
    "import os, sys, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(42); np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (9,5); plt.rcParams['axes.grid'] = True\n",
    "\n",
    "NB_DIR = Path.cwd()\n",
    "PROJECT_DIR = NB_DIR.parent if NB_DIR.name=='notebooks' else NB_DIR\n",
    "DATA_RAW = PROJECT_DIR / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_DIR / 'data' / 'processed'\n",
    "for p in [DATA_RAW, DATA_PROCESSED]: p.mkdir(parents=True, exist_ok=True)\n",
    "print('DATA_RAW:', DATA_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f577ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes: (421570, 5) (8190, 12) (45, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load raw files (train, features, stores)\n",
    "train_p = DATA_RAW / 'train.csv'\n",
    "feat_p  = DATA_RAW / 'features.csv'\n",
    "stores_p= DATA_RAW / 'stores.csv'\n",
    "assert train_p.exists(), f'Missing {train_p}'\n",
    "assert feat_p.exists(), f'Missing {feat_p}'\n",
    "assert stores_p.exists(), f'Missing {stores_p}'\n",
    "\n",
    "train = pd.read_csv(train_p, parse_dates=['Date'])\n",
    "features = pd.read_csv(feat_p, parse_dates=['Date'])\n",
    "stores = pd.read_csv(stores_p)\n",
    "print('Loaded shapes:', train.shape, features.shape, stores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8583337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged df shape: (421570, 20) Date range: 2010-02-05 00:00:00 -> 2012-10-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge & basic temporal features\n",
    "df = train.merge(stores, on='Store', how='left').merge(features, on=['Store','Date','IsHoliday'], how='left')\n",
    "df = df.sort_values(['Store','Dept','Date']).reset_index(drop=True)\n",
    "df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "print('Merged df shape:', df.shape, 'Date range:', df['Date'].min(), '->', df['Date'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046f78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarkDown cols: ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MarkDowns: imputar 0 y flags\n",
    "md_cols = [c for c in ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'] if c in df.columns]\n",
    "for c in md_cols:\n",
    "    df[c+'_missing'] = df[c].isna().astype(int)\n",
    "    df[c] = df[c].fillna(0.0)\n",
    "print('MarkDown cols:', md_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743296b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calendar sin/cos and holiday proximity by Store-Dept\n",
    "import numpy as np\n",
    "df['sin_week'] = np.sin(2*np.pi*df['Week']/52.0)\n",
    "df['cos_week'] = np.cos(2*np.pi*df['Week']/52.0)\n",
    "df['sin_month'] = np.sin(2*np.pi*df['Month']/12.0)\n",
    "df['cos_month'] = np.cos(2*np.pi*df['Month']/12.0)\n",
    "\n",
    "# Prev/Next holiday flags per Store-Dept\n",
    "df['IsHolidayPrevWeek'] = df.groupby(['Store','Dept'])['IsHoliday'].shift(1).fillna(0).astype(int)\n",
    "df['IsHolidayNextWeek'] = df.groupby(['Store','Dept'])['IsHoliday'].shift(-1).fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cae1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lags y rolling (basados en valores pasados): lag_1, lag_2, lag_52, roll_mean_4, roll_mean_13\n",
    "grp = df.groupby(['Store','Dept'], group_keys=False)\n",
    "df['lag_1'] = grp['Weekly_Sales'].shift(1)\n",
    "df['lag_2'] = grp['Weekly_Sales'].shift(2)\n",
    "df['lag_52'] = grp['Weekly_Sales'].shift(52)\n",
    "\n",
    "# Para los rolling, aplicar por grupo y asignar directamente para evitar problemas de índices\n",
    "df['roll_mean_4'] = grp['Weekly_Sales'].shift(1).groupby([df['Store'], df['Dept']]).rolling(4, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "df['roll_mean_13'] = grp['Weekly_Sales'].shift(1).groupby([df['Store'], df['Dept']]).rolling(13, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "# Alternativamente, para mayor robustez y claridad, se puede usar transform:\n",
    "# df['roll_mean_4'] = grp['Weekly_Sales'].shift(1).groupby([df['Store'], df['Dept']]).transform(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "# df['roll_mean_13'] = grp['Weekly_Sales'].shift(1).groupby([df['Store'], df['Dept']]).transform(lambda x: x.rolling(13, min_periods=1).mean())\n",
    "\n",
    "# Flags para lags/rollings faltantes\n",
    "for c in ['lag_1','lag_2','lag_52','roll_mean_4','roll_mean_13']:\n",
    "    df[c+'_missing'] = df[c].isna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a867072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categóricos: one-hot Type, and create store_idx, dept_idx for embeddings\n",
    "df['Type'] = df['Type'].astype('category')\n",
    "df = pd.get_dummies(df, columns=['Type'], drop_first=False)\n",
    "store_map = {v:i for i,v in enumerate(sorted(df['Store'].unique()))}\n",
    "dept_map  = {v:i for i,v in enumerate(sorted(df['Dept'].unique()))}\n",
    "df['store_idx'] = df['Store'].map(store_map).astype(int)\n",
    "df['dept_idx'] = df['Dept'].map(dept_map).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178dbfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test shapes: (338738, 45) (41369, 45) (41463, 45)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Splits by time (reuse existing metadata if present)\n",
    "meta_prev = DATA_PROCESSED / 'metadata.json'\n",
    "if meta_prev.exists():\n",
    "    import json\n",
    "    mp = json.load(open(meta_prev))\n",
    "    train_end = mp.get('train_end'); valid_end = mp.get('valid_end')\n",
    "    from pandas import to_datetime\n",
    "    train_end = to_datetime(train_end); valid_end = to_datetime(valid_end)\n",
    "else:\n",
    "    dates = np.sort(df['Date'].unique())\n",
    "    train_end = dates[int(0.8*len(dates))]\n",
    "    valid_end = dates[int(0.9*len(dates))]\n",
    "train_mask = df['Date'] <= train_end\n",
    "valid_mask = (df['Date'] > train_end) & (df['Date'] <= valid_end)\n",
    "test_mask = df['Date'] > valid_end\n",
    "df_train = df[train_mask].copy(); df_valid = df[valid_mask].copy(); df_test = df[test_mask].copy()\n",
    "print('Train/Valid/Test shapes:', df_train.shape, df_valid.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfacd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\byed2\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\byed2\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados los CSVs procesados v2 y el parquet de embeddings.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selección de características v2 y guardado robusto de imputación+escalado como CSVs v2\n",
    "target_col = 'Weekly_Sales'\n",
    "base_cols = ['Store','Dept','Size','Temperature','Fuel_Price','CPI','Unemployment','IsHoliday','Year','Month','Week','Quarter','sin_week','cos_week','sin_month','cos_month','IsHolidayPrevWeek','IsHolidayNextWeek'] + [c for c in df.columns if c.startswith('Type_')]\n",
    "lag_cols = ['lag_1','lag_2','lag_52','roll_mean_4','roll_mean_13']\n",
    "md_flag_cols = [c for c in df.columns if c.endswith('_missing') and c.startswith('MarkDown')]\n",
    "feature_cols_v2 = [c for c in (base_cols + md_flag_cols + lag_cols + [c+'_missing' for c in lag_cols]) if c in df.columns]\n",
    "\n",
    "# Construir X/y e imputar+escalar usando estadísticas del train\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "imp = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_tr = df_train[feature_cols_v2].copy()\n",
    "X_va = df_valid[feature_cols_v2].copy()\n",
    "X_te = df_test[feature_cols_v2].copy()\n",
    "# Ajustar imputador+escalador en train\n",
    "X_tr_imp = pd.DataFrame(imp.fit_transform(X_tr), columns=feature_cols_v2, index=X_tr.index)\n",
    "X_tr_scl = pd.DataFrame(scaler.fit_transform(X_tr_imp), columns=feature_cols_v2, index=X_tr_imp.index)\n",
    "X_va_scl = pd.DataFrame(scaler.transform(pd.DataFrame(imp.transform(X_va), columns=feature_cols_v2, index=X_va.index)), columns=feature_cols_v2, index=X_va.index)\n",
    "X_te_scl = pd.DataFrame(scaler.transform(pd.DataFrame(imp.transform(X_te), columns=feature_cols_v2, index=X_te.index)), columns=feature_cols_v2, index=X_te.index)\n",
    "\n",
    "def save_proc(Xs, ys, path):\n",
    "    out = Xs.copy()\n",
    "    out[target_col] = ys.values\n",
    "    out.to_csv(path, index=False)\n",
    "\n",
    "save_proc(X_tr_scl, df_train[target_col], DATA_PROCESSED / 'train_processed_v2.csv')\n",
    "save_proc(X_va_scl, df_valid[target_col], DATA_PROCESSED / 'valid_processed_v2.csv')\n",
    "save_proc(X_te_scl, df_test[target_col], DATA_PROCESSED / 'test_processed_v2.csv')\n",
    "\n",
    "# Guardar parquet para embeddings con columnas cont_* y store_idx/dept_idx\n",
    "cont_cols = [c for c in feature_cols_v2 if not c.startswith('Type_') and c not in ['Store','Dept','store_idx','dept_idx']]\n",
    "pack_tr = pd.DataFrame({'store_idx': df_train['store_idx'].values, 'dept_idx': df_train['dept_idx'].values, target_col: df_train[target_col].values})\n",
    "\n",
    "# Solución: aplicar imputador y escalador SOLO sobre las columnas continuas, usando el mismo orden y columnas que en el fit\n",
    "# Extraer las columnas continuas del set de entrenamiento ya imputado y escalado\n",
    "cont_scaled_tr = X_tr_scl[cont_cols].copy()\n",
    "cont_scaled_tr.columns = [f'cont_{i}' for i in range(len(cont_cols))]\n",
    "cont_scaled_tr.index = df_train.index\n",
    "\n",
    "pack_tr = pd.concat([pack_tr, cont_scaled_tr], axis=1)\n",
    "pack_tr['y_log1p'] = np.log1p(pack_tr[target_col])\n",
    "pack_tr.to_parquet(DATA_PROCESSED / 'train_embeddings_v2.parquet', index=False)\n",
    "\n",
    "# Guardar metadatos\n",
    "meta_v2 = {'train_end': str(pd.to_datetime(df_train['Date'].max()).date()), 'valid_end': str(pd.to_datetime(df_valid['Date'].max()).date()), 'feature_cols_v2': feature_cols_v2}\n",
    "import json\n",
    "json.dump(meta_v2, open(DATA_PROCESSED / 'metadata_v2.json','w'), indent=2)\n",
    "print('Guardados los CSVs procesados v2 y el parquet de embeddings.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef5405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_processed_v2.csv NaNs count by col:\n",
      " OK - no NaNs\n",
      "valid_processed_v2.csv NaNs count by col:\n",
      " OK - no NaNs\n",
      "test_processed_v2.csv NaNs count by col:\n",
      " OK - no NaNs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sanity check: ensure no NaNs remain in processed csvs\n",
    "for p in ['train_processed_v2.csv','valid_processed_v2.csv','test_processed_v2.csv']:\n",
    "    dfp = pd.read_csv(DATA_PROCESSED / p)\n",
    "    na = dfp.isna().sum()\n",
    "    na = na[na>0]\n",
    "    print(p, 'NaNs count by col:\\n', na if not na.empty else 'OK - no NaNs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbf131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
